### README.md

```markdown
# ğŸ§  Autism Behavior Detection from Videos using ML & Computer Vision

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![OpenCV](https://img.shields.io/badge/OpenCV-enabled-green)](https://opencv.org/)
[![MediaPipe](https://img.shields.io/badge/MediaPipe-Google-red)](https://mediapipe.dev/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

This project uses **computer vision** and **machine learning** to detect early signs of **autism spectrum behaviors** in toddlers through **video analysis**.

It identifies:
- ğŸ‘ï¸ **Eye Contact Issues**
- ğŸ” **Repetitive Motions**
- ğŸ§â€â™‚ï¸ **Social Interaction Deficits**

---

## ğŸš€ Features

- Extracts facial, gaze, and body keypoints from videos using **MediaPipe**
- Trains separate models for each behavior category
- Detects multiple behaviors from new videos
- Generates structured reports and visualizations
- Modular pipeline â€” easy to improve or expand
- Streamlit-based interface for testing and visualization

---

## ğŸ“ Project Structure


AUTISMDETECT/
â”œâ”€â”€ app.py                      # Streamlit app interface
â”œâ”€â”€ dataset/                    # Raw labeled videos
â”‚   â”œâ”€â”€ repetitive\_motion/
â”‚   â””â”€â”€ social\_deficit/
â”œâ”€â”€ features/                   # Extracted feature CSVs
â”‚   â”œâ”€â”€ repetitive\_motion/
â”‚   â””â”€â”€ social\_deficit/
â”œâ”€â”€ models/                     # Trained classifiers (.pkl)
â”œâ”€â”€ reports/                    # Output JSON reports
â”œâ”€â”€ scripts/                    # Main scripts
â”‚   â”œâ”€â”€ batch\_feature\_extractor.py
â”‚   â”œâ”€â”€ train\_classifiers.py
â”‚   â””â”€â”€ test\_video.py
â”œâ”€â”€ venv/                       # Virtual environment (optional)
â””â”€â”€ requirements.txt


---

## ğŸ“¦ Setup Instructions

### 1. Clone the Repository or Download Project

```bash
cd path/to/project
````

### 2. Create and Activate Virtual Environment

```bash
python -m venv venv
venv\Scripts\activate       # On Windows
# OR
source venv/bin/activate    # On Mac/Linux
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

---

## ğŸ§  Full Pipeline

### âœ… Step 1: Feature Extraction

Run this to extract facial/pose keypoints and save to CSV:

```bash
python scripts/batch_feature_extractor.py
```

---

### âœ… Step 2: Train Behavior Classifiers

Train separate ML models for each category:

```bash
python scripts/train_classifiers.py
```

Models are saved in `models/`.

---

### âœ… Step 3: Test a Combined Behavior Video

Run detection for a new video:

```bash
python scripts/test_video.py --video_path dataset/repetitive_motion/handflapping.mp4
```

Outputs:

* ğŸ“„ JSON report â†’ `reports/<video>_report.json`
* ğŸ§  (Optional) visuals â†’ `visuals/<video>_annotated.mp4`

---

## âœ… Streamlit Interface (GUI)

A friendly interface for uploading a video and seeing results.

### â–¶ï¸ Launch it:

```bash
streamlit run app.py
```

### ğŸ–¼ Features:

* Upload `.mp4` video
* Extract features and run predictions
* View report in JSON
* View pose/gaze plots (optional)
* Risk level calculation

---

## ğŸ“„ Sample Report Output

```json
{
  "eye_contact_issue": true,
  "repetitive_motion": false,
  "social_deficit": true,
  "autism_risk_level": "Moderate"
}
```

---

## ğŸ“Œ Future Improvements

* Add speech & emotion detection
* Real-time detection via webcam
* Deep learning models (CNN, LSTM)
* Auto-labeling with feedback loop

---

## ğŸ“œ License

This project is licensed under the [MIT License](LICENSE).

---

## ğŸ™Œ Acknowledgements

* [MediaPipe by Google](https://mediapipe.dev/)
* [Scikit-learn](https://scikit-learn.org/)
* [OpenCV](https://opencv.org/)

---

## âœ‰ï¸ Contact

**Project by:** Sukhad Kaur
ğŸ“§ Email: [sukhadkaur141005@gmail.com]

```
